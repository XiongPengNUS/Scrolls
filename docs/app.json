[{"name": "app.py", "content": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom, norm, t, linregress\n\nfrom shiny import reactive\nfrom shiny.express import render, ui, input\n\n\nchd_style = 'color:white; background:#007bc2 !important;'\n\n\ndef ui_block(string, btype):\n\n    return ui.markdown(f'<div class=\"alert alert-block alert-{btype}\">\\n{string}</div>')\n\n\ndef ci_equations(param, stats):\n\n    if param == 'mean':\n        center = '\\\\bar{x}'\n        center_eq = '=\\\\frac{1}{n}\\\\sum_{i=1}^nx_i'\n        if stats == 'sigma':\n            se = '\\\\frac{\\\\sigma}{\\\\sqrt{n}}' \n            se_text = '<li>Population standard deviation $\\\\sigma$</li>\\n'\n        else:\n            se = '\\\\frac{s}{\\\\sqrt{n}}'\n            se_text = '<li>Sample standard deviation $s$.</li>\\n'\n    else:\n        center = '\\\\hat{p}'\n        center_eq = ''\n        se = '\\\\sqrt{\\\\frac{\\\\hat{p}(1-\\\\hat{p})}{n}}'\n        se_text = ''\n    if stats == 'sigma':\n        ppf = 'z_{\\\\alpha/2}'\n        distr, dof = 'standard normal ', ''\n    else:\n        ppf = 't_{\\\\alpha/2, n-1}'\n        distr, dof = '$t$-', ' with the degree of freedom to be $n-1$'\n\n    return ('$$\\n'\n            f'{center} \\\\pm {ppf} \\\\cdot {se}\\n'\n            '$$\\n'\n            '<ul>\\n'\n            f'<li>Sample {param}: ${center}{center_eq}$.</li>\\n'\n            f'<li>Cut-off value ${ppf}$ as the $(1-\\\\alpha/2)$th percentile of the {distr}'\n            f'distribution{dof}.</li>\\n'\n            f'{se_text}'\n            '<li>Sample size $n$.</li>\\n'\n            '</ul>\\n')\n\n\ndef mysterious(size):\n\n    x = np.random.rand(size)\n    y = ((1.2 - 0.2*x) * np.sin(11*x) + 4*x) * 4 + np.random.randn(size)\n\n    return pd.DataFrame({'y': y, 'x': x})\n\n\nmath_tag = (\n    ui.tags.link(\n        rel=\"stylesheet\",\n        href=\"https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css\"\n    ),\n    ui.tags.script(src=\"https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js\"),\n    ui.tags.script(src=\"https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js\"),\n    ui.tags.script(\"\"\"\n        document.addEventListener('DOMContentLoaded', function() {\n            renderMathInElement(document.body, {\n                    delimiters: [\n                    {left: \"$$\", right: \"$$\", display: true},\n                    {left: \"$\", right: \"$\", display: false},\n                ]\n            });\n        });\n    \"\"\")\n)\n\nwith ui.tags.head():\n    math_tag\n\nui.markdown('# INotes')\nui.markdown('---')\n\nwith ui.navset_pill_list(id=\"selected_navset_pill_list\", widths=(3, 9), well=False):\n    with ui.nav_panel(\"Probability Theory\"):\n        ui.markdown('### Discrete Random Variables')\n        ui.markdown(('A random variable $X$ is defined to be discrete if its possible '\n                     'outcomes are finite or countable. Examples of distributions of '\n                     'discrete random variables are discrete uniform distribution (i.e., '\n                     'outcome of rolling an even die), Bernoulli distribution (i.e., the '\n                     'preference of a randomly selected customer for Coke or Pepsi), '\n                     'Binomial distribution (i.e., the number of customers who prefer Coke '\n                     'over Pepsi among 10 randomly selected customers), and Poisson '\n                     'distribution (i.e., The number of patients arriving in an emergency '\n                     'room within a fixed time interval) etc. '))\n\n        ui_block(('<b>Notes: </b> For a discrete random variable $X$ with $k$ possible '\n                  'outcomes $x_j$, $j=1, 2, ..., k$, the <b>probability mass function '\n                  '(PMF)</b> is given as:\\n'\n                  '$$\\n'\n                  '\\\\begin{align*}\\n'\n                  'P(X=x_j) = p_j \\\\text{~~~~for each~}j=1, 2, ..., k, \\n'\n                  '\\\\end{align*}'\n                  '$$\\n'\n                  'where $p_j$ is the probability of the outcome $x_j$ and all $p_j$ '\n                  'must satisfy \\n'\n                  '$$\\n'\n                  '\\\\begin{cases}\\n'\n                  '0\\\\leq p_j \\\\leq 1 \\\\text{~~~~for each }j=1, 2, ..., k, \\\\\\\\ \\n'\n                  '\\\\sum_{j=1}^kp_j = 1. \\n'\n                  '\\\\end{cases}\\n'\n                  '$$\\n'), 'danger')\n\n        with ui.card(height='850px'):\n            ui.card_header('Binomial distribution', style=chd_style)\n            with ui.layout_sidebar():\n                with ui.sidebar(bg='#f8f8f8', width='360px'):  \n                    ui.input_slider(id='drv_prop', label='Probability of success:',\n                                    min=0.05, max=0.95, value=0.40, step=0.01)\n                    ui.input_slider(id='drv_num', label='Number of experiments:',\n                                    min=5, max=100, value=25, step=1)\n                \n                    @render.ui\n                    @reactive.event(input.drv_num)\n                    def obs_count():\n                        num = input.drv_num()\n                        return ui.input_slider(id='drv_m', label=ui.HTML('Number of successes:'),\n                                               min=0, max=num, value=num//2, step=0.1)\n                    \n                    ui.markdown('---')\n\n                    @render.ui\n                    @reactive.event(input.drv_prop, input.drv_num, input.drv_m)\n                    def update_dvr_code():\n\n                        p = input.drv_prop()\n                        n = input.drv_num()\n                        m = input.drv_m()\n                        pmf = binom.pmf(m, n, p)\n                        cdf = binom.cdf(m, n, p)\n\n                        return ui.markdown(('```python\\n'\n                                            f'binom.pmf({m}, {n}, {p})\\n'\n                                            '```\\n\\n'\n                                            f'<code>{pmf}</code>\\n'\n                                            '```python\\n'\n                                            f'binom.cdf({m}, {n}, {p})\\n'\n                                            '```\\n\\n'\n                                            f'<code>{cdf}</code>\\n'))\n\n                ui.markdown(('The binomial distribution is a discrete probability distribution, '\n                             'defined to be the number of successes in a sequence of $n$ '\n                             'independent experiments, and each experiment has a Boolean valued '\n                             'outcome: success (with probability $p$) or failure (with '\n                             'probability $1-p$). The graphs below show the PMF and CDF of '\n                             'such a binomial distributed random variable $X$.'))\n\n                @render.plot\n                @reactive.event(input.drv_prop, input.drv_num, input.drv_m)\n                def update_binom_plot():\n                    p = input.drv_prop()\n                    n = input.drv_num()\n                    m = input.drv_m()\n                    x1 = np.arange(0, np.floor(m)+1)\n                    x2 = np.arange(np.floor(m)+1, n+1)\n                    pmf1 = binom.pmf(x1, n, p)\n                    pmf2 = binom.pmf(x2, n, p)\n                    pmf_m = binom.pmf(m, n, p)\n                    xs = np.arange(0, n+0.001, 0.001)\n                    cdf = binom.cdf(xs, n, p)\n                    cdf_m = binom.cdf(m, n, p)\n\n                    fig, ax = plt.subplots(2, 1)\n                    ax[0].vlines(x1, 0, pmf1, linewidth=2, color='b',\n                                 label=f'Probability $P(X\\\\leq{m})$')\n                    ax[0].vlines(x2, 0, pmf2, linewidth=2, color='gray',\n                                 label=f'Probability $P(X>{m})$')\n                    ax[0].scatter(m, pmf_m, s=40, c='r', alpha=0.5,\n                                 label=f'$P(X={m}) = {pmf_m.round(4)}$')\n                    ax[0].grid()\n                    if p > 0.5:\n                        ax[0].legend(fontsize=11, loc='upper left')\n                    else:\n                        ax[0].legend(fontsize=11, loc='upper right')\n                    ax[0].set_ylabel('PMF', fontsize=11)\n                    ax[1].plot(xs, cdf, color='b', linewidth=1.5)\n                    ax[1].scatter(m, cdf_m, s=40, c='r', alpha=0.5,\n                                  label=f'$P(X\\\\leq {m}) = {cdf_m.round(4)}$')\n                    ax[1].grid()\n                    ax[1].legend(fontsize=11)\n                    ax[1].set_ylabel('CDF', fontsize=11)\n                    ax[1].set_xlabel('Number of successes', fontsize=11)\n                \n                    return fig\n        \n        ui.markdown('<br>')\n        ui.markdown('### Continuous Random Variables')\n        ui.markdown(('A variable $X$ is a **continuous random variable** if takes all values in '\n                     'an interval of numbers. Random variables following uniform, normal '\n                     '(Gaussian) and exponential distributions are all continuous variables. \\n'\n                     'For continuous random variables, there is no PMF as the discrete random '\n                     'variables, because $P(X=x)=0$ for all values of $x$. Only intervals of '\n                     'values have positive probabilities, such as $P(0 \\\\leq x \\\\leq 10)$. The '\n                     'CDF for a continuous random variable has the same definition as the '\n                     'discrete case, which is $F(x)=P(X\\\\leq x)$. Based on the CDF, we have other '\n                     'definitions listed as follows.'))\n        ui_block(('<b>Notes: </b> Let $F(x) = P(X\\\\leq x)$ be the CDF of a continuous random '\n                  'variable $X$, then\\n'\n                  '<li> The derivative $f(x) = \\\\frac{d F(x)}{dx}$ of the CDF $F(x)$ is called '\n                  'the <b>probability density function (PDF)</b> of $X$. This definition also '\n                  'implies that $F(x) = \\\\int_{-\\\\infty}^{x}f(t)dt$.</li> '\n                  '<li> The inverse of CDF $F(x)$, denoted by $F^{-1}(q)$, is called the '\n                  '<b>Percent Point Function (PPF)</b>, where $q$ is the given cumulative '\n                  'probability. This function is sometimes referred to as the <b>inverse '\n                  'cumulative distribution function</b> or the <b>quantile function</b>.'),\n                  'danger')\n        \n        with ui.card(height='650px'):\n            ui.card_header('Standard normal distribution', style=chd_style)\n            with ui.layout_sidebar():\n                with ui.sidebar(bg='#f8f8f8', width='360px'):  \n                    ui.input_slider(id='crv_x', label='Value of the random variable:',\n                                    min=-3.5, max=3.5, value=0.60, step=0.01)\n                    \n                    ui.markdown('---')\n\n                    @render.ui\n                    @reactive.event(input.crv_x)\n                    def update_cvr_code():\n\n                        x = input.crv_x()\n                        pdf = norm.pdf(x)\n                        cdf = norm.cdf(x)\n\n                        return ui.markdown(('```python\\n'\n                                            f'norm.pdf({x}, loc=0, scale=1)\\n'\n                                            '```\\n\\n'\n                                            f'<code>{pdf}</code>\\n'\n                                            '```python\\n'\n                                            f'norm.cdf({x}, loc=0, scale=1)\\n'\n                                            '```\\n\\n'\n                                            f'<code>{cdf}</code>\\n'\n                                            '```python\\n'\n                                            f'norm.ppf({cdf.round(4)}, loc=0, scale=1)\\n'\n                                            '```\\n\\n'\n                                            f'<code>{x}</code>\\n'))\n                \n                ui.markdown(('The standard normal distribution is a normal distribution with '\n                             'zero mean and unit standard deviation. Such a distribution is used '\n                             'to illustrate the concepts of PDF, CDF, PPF, in the following '\n                             'figure.'))\n\n                @render.plot\n                @reactive.event(input.crv_x)\n                def update_norm_plot():\n                    \n                    step = 0.001\n                    x = input.crv_x()\n                    xs = np.arange(-3.5, 3.5+step, step)\n                    pdf = norm.pdf(xs)\n                    \n                    fig, ax = plt.subplots()\n                    ax.plot(xs, pdf, color='k', linewidth=2.5, alpha=0.6, label='PDF curve')\n                    ax.fill_between(xs[xs<=x], y1=0, y2=pdf[xs<=x], color='b', alpha=0.4,\n                                    label=f'CDF $P(X \\\\leq {x}) = {norm.cdf(x).round(4)}$')\n                    ax.scatter(x, 0, s=40, c='r', alpha=0.5,\n                               label=f'PPF $F^{{-1}}({norm.cdf(x).round(4)}) = {x}$')\n                    ax.legend(fontsize=11, loc='upper right')\n                    ax.grid()\n                    ax.set_ylim([-0.03, 0.57])\n                    ax.set_xlabel('Random variable $X$', fontsize=11)\n                    ax.set_ylabel('PDF', fontsize=11)\n\n                    return fig\n\n    with ui.nav_panel(\"Random Sampling\"):\n        ui.markdown('### Distribution of Sample Means')\n        ui.markdown(('Let $\\\\{X_1, X_2, ..., X_n\\\\}$ be a random sample of size $n$, *i.e.* a '\n                     'a sequence of independent and identically distributed (i.i.d.) random '\n                     'variables drawn from a population with an expected value $\\\\mu$ and finite '\n                     'variance $\\\\sigma^2$, the sample mean is expressed as '\n                     '$\\\\bar{x} = \\\\frac{1}{n}\\\\sum_{i=1}^nx_i$. \\n'))\n        ui_block(('<b>Central Limit Theorem:</b> For a relatively large sample size, the random '\n                  'variable $\\\\bar{x} = \\\\frac{1}{n}\\\\sum_{i=1}^nx_i$ is approximately normally '\n                  'distributed, regardless of the distribution of the population. The '\n                  'approximation becomes better with increased sample size.'), 'danger')\n        ui.markdown(('The following experiments are conducted to verify the Central Limit '\n                     'Theorem (CLT). '))\n        \n        with ui.card(height='650px'):\n            ui.card_header('Sampling Distribution of the Mean', style=chd_style)\n            with ui.layout_sidebar():\n                with ui.sidebar(bg='#f8f8f8', width='360px'):  \n                    ui.input_slider(id='sm_size', label='Sample size:',\n                                    min=2, max=200, value=50, step=2)\n                    dists = ['Uniform distribution between 0 and 1',\n                             'Exponential distribution with unit mean',\n                             'Standard normal distribution',\n                             'Discrete uniform distribution for tossing a fair die',\n                             'Bernoulli distribution with the probability to be 0.5']\n                    funs = ['np.random.uniform(size=(1000, size))',\n                            'np.random.exponential(size=(1000, size))',\n                            'np.random.normal(size=(1000, size))',\n                            'np.random.randint(1, 7, size=(1000, size))',\n                            'np.random.binomial(1, 0.5, size=(1000, size))']\n                    means = [0.5, 1, 0, 3.5, 0.5]\n                    stds = [1/3**0.5/2, 1, 1, np.arange(1, 7).std(), 0.5]\n                    ui.input_select(id='sm_dtype', label=ui.markdown('Distribution of $X_i$'),\n                                    choices=dists, selected=dists[0])\n                    ui.markdown('---')\n                    \n                    @render.data_frame\n                    @reactive.event(input.sm_size, input.sm_dtype)\n                    def update_sm_table():\n\n                        np.random.seed(0)\n                        size = input.sm_size()\n                        dtype = input.sm_dtype()\n                        idx = dists.index(dtype)\n                    \n                        samples = eval(funs[idx])\n                        mean_exact = np.round(means[idx], 4)\n                        std_exact = np.round(stds[idx] / size**0.5, 4)\n                        mean_smp = np.round(samples.mean(), 4)\n                        std_smp = np.round(samples.mean(axis=1).std(ddof=1), 4)\n\n                        #return pd.DataFrame(np.random.rand(3, 5), columns=list('abcde'))\n\n                        return pd.DataFrame({' ': ['Mean', 'STD'],\n                                             'Exact': [mean_exact, std_exact],\n                                             'Statistics': [mean_smp, std_smp]})\n\n                @render.plot\n                @reactive.event(input.sm_size, input.sm_dtype)\n                def update_sm_plot():\n                    \n                    np.random.seed(0)\n                    size = input.sm_size()\n                    dtype = input.sm_dtype()\n                    idx = dists.index(dtype)\n                    \n                    samples = eval(funs[idx])\n                    mu = means[idx]\n                    sigma = stds[idx]\n                    xs = samples[0].copy()\n                    xs.sort()\n                    xm = samples.mean(axis=1)\n                    xm.sort()\n                    \n                    fig, ax = plt.subplots(2, 2)\n                    ax[0, 0].hist(xs, bins=25, color='b', alpha=0.5)\n                    ax[0, 0].set_xlabel('Sample data value')\n                    ax[0, 0].set_ylabel('Frequency')\n                    ax[0, 0].set_title('Histogram of a sample', fontsize=10)\n                    ax[0, 1].scatter(norm.ppf((np.arange(size)+0.5) / size),\n                                     (xs - mu)/sigma, color='b', alpha=0.3)\n                    ax[0, 1].plot([-3, 3], [-3, 3], linewidth=2, color='r', alpha=0.5)\n                    ax[0, 1].set_xlabel('Standard normal quantiles')\n                    ax[0, 1].set_ylabel('Ordered standard values')\n                    ax[0, 1].set_title('Q-Q plot of the sample data', fontsize=10)\n                    ax[1, 0].hist(xm, bins=25, color='b', alpha=0.5)\n                    ax[1, 0].set_xlabel('Sample mean value')\n                    ax[1, 0].set_ylabel('Frequency')\n                    ax[1, 0].set_title('Histogram of 1000 sample means', fontsize=10)\n                    ax[1, 1].scatter(norm.ppf((np.arange(1000)+0.5) / 1000),\n                                     (xm - mu)/sigma*(size**0.5), color='b', alpha=0.3)\n                    ax[1, 1].plot([-3, 3], [-3, 3], linewidth=2, color='r', alpha=0.5)\n                    ax[1, 1].set_xlabel('Standard normal quantiles')\n                    ax[1, 1].set_ylabel('Ordered standard values')\n                    ax[1, 1].set_title('Q-Q plot of 1000 sample means', fontsize=10)\n\n                    return fig\n\n    with ui.nav_panel(\"Confidence Intervals\"):\n        ui.markdown('### Introduction to Confidence Intervals')\n        ui.markdown(('Confidence interval provides a range of plausible values for the unknown '\n                     'population parameter (such as the mean). The probability, or confidence '\n                     'that the parameter lies in the confidence interval (i.e., that the '\n                     'confidence interval contains the parameter), is called the confidence '\n                     'level, denoted by $1-\\\\alpha$ in this lecture. If $1-\\\\alpha=95$%, for '\n                     'instance, we are $95$% confident that the true population parameter lies '\n                     'within the confidence interval. \\n'))\n    \n        with ui.card(height='700px'):\n            ui.card_header('Experiments on Confidence Intervals', style=chd_style)\n            with ui.layout_sidebar():\n                with ui.sidebar(bg='#f8f8f8', width='360px'):  \n                    ui.input_slider(id='ci_size', label='Sample size:',\n                                    min=5, max=500, value=50, step=5)\n                    ui.input_slider(id='ci_level', label='Confidence level',\n                                    min=0.85, max=0.99, value=0.95, step=0.01)\n                    std_info = ['Known standard deviation', 'Unknown standard deviation']\n                    ui.input_select(id='ci_info', label='Population information',\n                                    choices=std_info, selected=std_info[0])\n\n                ui.markdown(('In the following example, we repeat a sampling experiment 100 '\n                             'times, and in each experiment, a sample with size $n$ is randomly '\n                             'selected from a population uniformly distributed between 0 and 1. '\n                             'The confidence interval for estimating the population mean is '\n                             'calculated using the sample data and compared with the true '\n                             'population parameter.'))\n                \n                @render.plot\n                @reactive.event(input.ci_size, input.ci_level, input.ci_info)\n                def update_ci_plot():\n                    \n                    np.random.seed(0)\n                    size = input.ci_size()\n                    level = input.ci_level()\n                    info = input.ci_info()\n\n                    samples = np.random.rand(100, size)\n                    if info == std_info[0]:\n                        moe = norm.ppf(0.5 + level/2) * (1/2/3**0.5 / size**0.5) * np.ones(100)\n                    else:\n                        moe = norm.ppf(0.5 + level/2) * samples.std(axis=1, ddof=1) / size**0.5\n\n                    fig, ax = plt.subplots()\n                    idx = np.arange(1, 101)\n                    means = samples.mean(axis=1)\n                    ax.plot([-10, 110], [0.5, 0.5], color='b', linewidth=1.5, alpha=0.5)\n                    is_out = (0.5 - moe > means) | (0.5 + moe < means)\n                    ax.scatter(idx[~is_out], means[~is_out], s=10, c='k')\n                    ax.scatter(idx[is_out], means[is_out], s=10, c='r')\n                    ax.errorbar(idx[~is_out], means[~is_out],\n                                yerr=moe[~is_out], capsize=2, color='k', fmt='none')\n                    ax.errorbar(idx[is_out], means[is_out],\n                                yerr=moe[is_out], capsize=2, color='r', fmt='none')\n                    ax.set_xlabel('Experiments')\n                    ax.set_xlim([-2, 102])\n                    dev = (1/2/3**0.5 / size**0.5)\n                    ax.set_ylim([0.5-dev*6, 0.5+dev*6])\n                    ax.grid()\n\n                    return fig\n            \n                ui.markdown(('The definition of the confidence interval suggests that it covers '\n                             'the true value of the population mean with a probability of '\n                             '$1-\\\\alpha$. As a result, in the graph above, if $1-\\\\alpha=95$%,'\n                             ' there are roughly (not always) $95$% of intervals (black lines) '\n                             'capture the true population mean, while the remaining $5$% (red '\n                             'lines) cases the population mean may fall out of the interval.'))\n\n        ui.markdown('<br>')\n        ui.markdown('### Calculations of Confidence Intervals')\n        ui.markdown(('The equation for calculating the confidence interval when estimating the '\n                     '**population mean** is presented below. The calculation is based on the '\n                     'sample mean $\\\\bar{x}$ and the sample standard deviation $s$.'))\n        ui_block(ci_equations('mean', 's'), 'danger')\n        ui.markdown('The equation for calculating the confidence interval of the **population '\n                    'proportion** is given as follows. ')\n        ui_block(ci_equations('proportion', 'sigma'), 'danger')\n\n        with ui.card(height='550px'):\n            ui.card_header('Sample size for polling', style=chd_style)\n            with ui.layout_sidebar():\n                with ui.sidebar(bg='#f8f8f8', width='360px'):  \n                    ui.input_slider(id='polling_size', label='Sample size:',\n                                    min=50, max=2500, value=1000, step=50)\n                    ui.input_slider(id='polling_prop', label='Support rate of a candidate:',\n                                    min=0.1, max=0.9, value=0.35, step=0.05)\n                    ui.input_slider(id='polling_level', label='Confidence level:',\n                                    min=0.85, max=0.99, value=0.95, step=0.01)\n            \n                ui.markdown(('Political polling is usually used to predict the results of an '\n                             'election. In this example, we focus on how 1) the sample size $n$; '\n                             '2) the support rate $p$ of a candidate in the population; and 3) '\n                             'the confidence level $1-\\\\alpha$, affect the credibility of a poll, '\n                             'in terms of the margin of error.'))\n                \n                @render.plot\n                @reactive.event(input.polling_size, input.polling_prop, input.polling_level)\n                def update_polling_plot():\n\n                    size = input.polling_size()\n                    p = input.polling_prop()\n                    level = input.polling_level()\n\n                    sizes = np.arange(40, 2501)\n                    ci_curve = norm.ppf(0.5 + level/2) * ((p*(1-p))**0.5 / sizes**0.5)\n                    ci_max = norm.ppf(0.5 + level/2) * (0.5 / sizes**0.5)\n                    ci = norm.ppf(0.5 + level/2) * ((p*(1-p))**0.5 / size**0.5)\n\n                    fig, ax = plt.subplots()\n                    ax.plot(sizes, ci_curve*100, color='k', linewidth=1.5, alpha=0.55,\n                            label=f'Margin of error with $p={p}$')\n                    ax.plot(sizes, ci_max*100, \n                            color='m', linewidth=1.5, linestyle='--', alpha=0.55,\n                            label=f'Maximum margin of error with $p=0.5$')\n                    ax.scatter(size, ci*100, s=40, c='r', alpha=0.5)\n                    ax.plot([0, size, size], [ci*100, ci*100, 0], \n                            color='r', linestyle='--', alpha=0.5)\n                    ax.legend(fontsize=11)\n                    ax.grid()\n                    ax.set_xlabel('Sample size $n$', fontsize=11)\n                    ax.set_ylabel('Margin of error (in percentage)', fontsize=11)\n                    ax.set_xlim([0, 2580])\n                    ax.set_ylim([0, 16.8])\n                    \n                    return fig\n\n    with ui.nav_panel(\"Hypothesis Testing\"):\n        with ui.navset_card_underline(id=\"ht_estimate_type\"):\n            with ui.nav_panel(\"Population mean\"):\n                ui.markdown((\"**Step 1: Choose the hypotheses**<br>\"\n                             \"The first step in setting up a hypothesis test is to decide on \"\n                             \"the null hypothesis and the alternative hypothesis. <br><br>\"))\n                with ui.navset_card_underline(id=\"ht_mean_test_type\"):\n                    with ui.nav_panel(\"Left-tail test\"):\n                        ui_block(('$$\\n'\n                                  '\\\\begin{cases}\\n'\n                                  'H_0:~\\\\mu \\\\geq \\\\mu_0 \\\\\\\\ \\n'\n                                  'H_a:~\\\\mu < \\\\mu_0 \\n'\n                                  '\\\\end{cases}\\n'\n                                  '$$\\n'\n                                  '<ul>\\n'\n                                  '<li>$\\\\mu_0$ is the mean value assumed in the null '\n                                  'hypothesis for testing.</li>'\n                                  '</ul>'), 'danger')\n                    with ui.nav_panel(\"Right-tail test\"):\n                        ui_block(('$$\\n'\n                                  '\\\\begin{cases}\\n'\n                                  'H_0:~\\\\mu \\\\leq \\\\mu_0 \\\\\\\\ \\n'\n                                  'H_a:~\\\\mu > \\\\mu_0 \\n'\n                                  '\\\\end{cases}\\n'\n                                  '$$\\n'\n                                  '<ul>\\n'\n                                  '<li>$\\\\mu_0$ is the mean value assumed in the null '\n                                  'hypothesis for testing.</li>'\n                                  '</ul>'), 'danger')\n                    with ui.nav_panel(\"Two-tail test\"):\n                        ui_block(('$$\\n'\n                                  '\\\\begin{cases}\\n'\n                                  'H_0:~\\\\mu = \\\\mu_0 \\\\\\\\ \\n'\n                                  'H_a:~\\\\mu \\\\not= \\\\mu_0 \\n'\n                                  '\\\\end{cases}\\n'\n                                  '$$\\n'\n                                  '<ul>\\n'\n                                  '<li>$\\\\mu_0$ is the mean value assumed in the null '\n                                  'hypothesis for testing.</li>'\n                                  '</ul>'), 'danger')\n                ui.markdown(\"<br>\")\n                ui.markdown(\"**Step 2: Compute the test statistic**<br><br>\")\n                ui_block(('$$\\n'\n                          't_0 = \\\\frac{\\\\bar{x} - \\\\mu_0}{s/\\\\sqrt{n}}\\n'\n                          '$$\\n'\n                          '<ul>\\n'\n                          '<li> Sample mean $\\\\bar{x} = \\\\frac{1}{n}\\\\sum_{i=1}^nx_i$.</li>\\n'\n                          '<li>Sample standard deviation $s$.</li>\\n'\n                          '<li>Sample size $n$.</li>'\n                          '</ul>'), 'danger')\n                \n                ui.markdown(\"<br>\")\n                ui.markdown((\"**Step 3: Calculate the $P$-value**<br>\"\n                             \"The $P$-value of a hypothesis test is the probability of getting \"\n                             \"sample data at least as inconsistent with the null hypothesis \"\n                             \"(and supportive of the alternative hypothesis) as the sample data \"\n                             \"actually obtained. <br><br>\"))\n\n                with ui.card(height='400px'):\n                    ui.card_header('The $P$-value approach to hypothesis testing',\n                                   style=chd_style)\n                    with ui.layout_sidebar():\n                        with ui.sidebar(bg='#f8f8f8', width='360px'):\n                            ui.input_slider(id='t_test_value', label='Test statistic: value',\n                                            min=-4, max=4, value=-1.5, step=0.001)\n                            ui.input_slider(id='t_test_size', label='Sample size:',\n                                            min=5, max=200, value=25, step=1)\n                    \n                        @render.plot\n                        @reactive.event(input.ht_mean_test_type, \n                                        input.t_test_value, input.t_test_size)\n                        def update_t_test_type():\n                        \n                            test_type = input.ht_mean_test_type()\n                            stat = input.t_test_value()\n                            size = input.t_test_size()\n\n                            xs = np.arange(-4, 4.001, 0.001)\n                            pdf = t.pdf(xs, size-1)\n                            \n                            fig, ax = plt.subplots()\n                            ax.plot(xs, pdf, color='k', linewidth=1.5, alpha=0.6,\n                                    label='$t$-distribution PDF')\n                            ax.set_title(f'{test_type} with $t_0={stat}$', fontsize=11)\n                            ax.scatter(stat, t.pdf(stat, size-1), s=40, c='r', alpha=0.5)\n                            ax.plot([stat, stat], [0, t.pdf(stat, size-1)], \n                                    color='r', linestyle='--',\n                                    label=f'Test statistic $t_0={stat}$')\n\n                            if test_type == 'Left-tail test':\n                                xs_left = xs[xs<=stat]\n                                pdf_left = t.pdf(xs_left, size-1)\n                                cdf_left = t.cdf(stat, size-1).round(4)\n                                ax.fill_between(xs_left, y1=0, y2=pdf_left,\n                                                color='b', alpha=0.4,\n                                                label=f'$P$-value = {cdf_left}')\n                            elif test_type == 'Right-tail test':\n                                xs_right = xs[xs>=stat]\n                                pdf_right = t.pdf(xs_right, size-1)\n                                cdf_right = (1 - t.cdf(stat, size-1)).round(4)\n                                ax.fill_between(xs_right, y1=0, y2=pdf_right,\n                                                color='b', alpha=0.4,\n                                                label=f'$P$-value = {cdf_right}')\n                            else:\n                                xs_both = xs.copy()\n                                xs_both[abs(xs) <= abs(stat)] = np.nan\n                                pdf_both = t.pdf(xs_both, size-1)\n                                cdf_both = (2 * (1 - t.cdf(np.abs(stat), size-1))).round(4)\n                                ax.fill_between(xs_both, y1=0, y2=pdf_both,\n                                                color='b', alpha=0.4,\n                                                label=f'$P$-value = {cdf_both}')\n\n                            ax.set_xlabel('$t_0$ value', fontsize=11)\n                            ax.set_ylabel('PDF', fontsize=11)\n                            ax.legend(fontsize=11)\n                            ax.grid()\n                            ax.set_ylim([-0.03, 0.67])\n\n                            return fig\n                \n                ui.markdown(\"<br>\")\n                ui.markdown((\"**Step 4: Make a decision**<br>\"\n                             \"Given a significant level $\\\\alpha$, we draw conclusions from the \"\n                             \"$P$-value. \\n \"))\n                ui_block((\"<li> We reject the null hypothesis $H_0$ in favor of the alternative \"\n                          \"hypothesis $H_a$, if the $P$-value is lower than the selected \"\n                          \"significance level $\\\\alpha$.</li>\"\n                          \"<li> Otherwise, we do not reject the null hypothesis.</li>\"), 'danger')\n\n            with ui.nav_panel(\"Population proportion\"):\n                ui.markdown((\"**Step 1: Choose the hypotheses**<br>\"\n                             \"The first step in setting up a hypothesis test is to decide on \"\n                             \"the null hypothesis and the alternative hypothesis. <br><br>\"))\n                with ui.navset_card_underline(id=\"ht_prop_test_type\"):\n                    with ui.nav_panel(\"Left-tail test\"):\n                        ui_block(('$$\\n'\n                                  '\\\\begin{cases}\\n'\n                                  'H_0:~p \\\\geq p_0 \\\\\\\\ \\n'\n                                  'H_a:~p < p_0 \\n'\n                                  '\\\\end{cases}\\n'\n                                  '$$\\n'\n                                  '<ul>\\n'\n                                  '<li>$p_0$ is the proportion value assumed in the null '\n                                  'hypothesis for testing.</li>'\n                                  '</ul>'), 'danger')\n                    with ui.nav_panel(\"Right-tail test\"):\n                        ui_block(('$$\\n'\n                                  '\\\\begin{cases}\\n'\n                                  'H_0:~p \\\\leq p_0 \\\\\\\\ \\n'\n                                  'H_a:~p > p_0 \\n'\n                                  '\\\\end{cases}\\n'\n                                  '$$\\n'\n                                  '<ul>\\n'\n                                  '<li>$p_0$ is the proportion value assumed in the null '\n                                  'hypothesis for testing.</li>'\n                                  '</ul>'), 'danger')\n                    with ui.nav_panel(\"Two-tail test\"):\n                        ui_block(('$$\\n'\n                                  '\\\\begin{cases}\\n'\n                                  'H_0:~p = p_0 \\\\\\\\ \\n'\n                                  'H_a:~p \\\\not= p_0 \\n'\n                                  '\\\\end{cases}\\n'\n                                  '$$\\n'\n                                  '<ul>\\n'\n                                  '<li>$p_0$ is the proportion value assumed in the null '\n                                  'hypothesis for testing.</li>'\n                                  '</ul>'), 'danger')\n                ui.markdown(\"<br>\")\n                ui.markdown(\"**Step 2: Compute the test statistic**<br><br>\")\n                ui_block(('$$\\n'\n                          'z_0 = \\\\frac{\\\\hat{p} - p_0}{\\\\sqrt{p_0(1-p_0)/n}}\\n'\n                          '$$\\n'\n                          '<ul>\\n'\n                          '<li> Sample proportion $\\\\hat{p}$.</li>\\n'\n                          '<li>Sample size $n$.</li>'\n                          '</ul>'), 'danger')\n                \n                ui.markdown(\"<br>\")\n                ui.markdown((\"**Step 3: Calculate the $P$-value**<br>\"\n                             \"The $P$-value of a hypothesis test is the probability of getting \"\n                             \"sample data at least as inconsistent with the null hypothesis \"\n                             \"(and supportive of the alternative hypothesis) as the sample data \"\n                             \"actually obtained. <br><br>\"))\n\n                with ui.card(height='400px'):\n                    ui.card_header('The $P$-value approach to hypothesis testing',\n                                   style=chd_style)\n                    with ui.layout_sidebar():\n                        with ui.sidebar(bg='#f8f8f8', width='360px'):\n                            ui.input_slider(id='z_test_value', label='Test statistic: value',\n                                            min=-4, max=4, value=-1.5, step=0.001)\n                            ui.input_slider(id='z_test_size', label='Sample size:',\n                                            min=5, max=200, value=25, step=1)\n                    \n                        @render.plot\n                        @reactive.event(input.ht_prop_test_type, \n                                        input.z_test_value, input.z_test_size)\n                        def update_z_test_type():\n                        \n                            test_type = input.ht_prop_test_type()\n                            stat = input.z_test_value()\n\n                            xs = np.arange(-4, 4.001, 0.001)\n                            pdf = norm.pdf(xs)\n                            \n                            fig, ax = plt.subplots()\n                            ax.plot(xs, pdf, color='k', linewidth=1.5, alpha=0.6,\n                                    label='Standard normal distribution PDF')\n                            ax.set_title(f'{test_type} with $t_0={stat}$', fontsize=11)\n                            ax.scatter(stat, norm.pdf(stat), s=40, c='r', alpha=0.5)\n                            ax.plot([stat, stat], [0, norm.pdf(stat)], color='r', linestyle='--',\n                                    label=f'Test statistic $z_0={stat}$')\n\n                            if test_type == 'Left-tail test':\n                                xs_left = xs[xs<=stat]\n                                pdf_left = norm.pdf(xs_left)\n                                cdf_left = norm.cdf(stat).round(4)\n                                ax.fill_between(xs_left, y1=0, y2=pdf_left, color='b', alpha=0.4,\n                                                label=f'$P$-value = {cdf_left}')\n                            elif test_type == 'Right-tail test':\n                                xs_right = xs[xs>=stat]\n                                pdf_right = norm.pdf(xs_right)\n                                cdf_right = (1 - norm.cdf(stat)).round(4)\n                                ax.fill_between(xs_right, y1=0, y2=pdf_right, color='b', alpha=0.4,\n                                                label=f'$P$-value = {cdf_right}')\n                            else:\n                                xs_both = xs.copy()\n                                xs_both[abs(xs) <= abs(stat)] = np.nan\n                                pdf_both = norm.pdf(xs_both)\n                                cdf_both = (2 * (1 - norm.cdf(np.abs(stat)))).round(4)\n                                ax.fill_between(xs_both, y1=0, y2=pdf_both, color='b', alpha=0.4,\n                                                label=f'$P$-value = {cdf_both}')\n\n                            ax.set_xlabel('$z_0$ value', fontsize=11)\n                            ax.set_ylabel('PDF', fontsize=11)\n                            ax.legend(fontsize=11)\n                            ax.grid()\n                            ax.set_ylim([-0.03, 0.67])\n\n                            return fig\n                \n                ui.markdown(\"<br>\")\n                ui.markdown((\"**Step 4: Make a decision**<br>\"\n                             \"Given a significant level $\\\\alpha$, we draw conclusions from the \"\n                             \"$P$-value. \\n \"))\n                ui_block((\"<li> We reject the null hypothesis $H_0$ in favor of the alternative \"\n                          \"hypothesis $H_a$, if the $P$-value is lower than the selected \"\n                          \"significance level $\\\\alpha$.</li>\"\n                          \"<li> Otherwise, we do not reject the null hypothesis.</li>\"), 'danger')\n    \n    with ui.nav_panel(\"Explanatory Modeling: Regression\"):\n        ui.markdown('### Notations:')\n        ui.markdown(('A linear regression model can be written as'\n                     '$$\\n'\n                     'y = \\\\beta_0 + \\\\beta_1x_1 + \\\\beta_2x_2 + \\\\cdots + \\\\beta_px_p + u\\n'\n                     '$$\\n'\n                     'where $y$ is the dependent variable, $x_1$, $x_2$, ..., $x_p$ are ' \n                     'independent variables, the term $u$ represents the random error, and '\n                     '$\\\\beta_0$, $\\\\beta_1$, $\\\\beta_2$, ..., $\\\\beta_p$ are model parameters.'))\n        with ui.card(height='650px'):\n            ui.card_header('Illustration of notations', style=chd_style)\n            with ui.layout_sidebar():\n                with ui.sidebar(bg='#f8f8f8', width='360px'):  \n                    notations = ['Population regression function',\n                                 'Sample regression function',\n                                 'Residuals']\n                    ui.input_selectize(id='display_reg', label='Notations',\n                                    choices=notations, selected=notations[0], multiple=True)\n                    ui.input_action_button(id='data_gen', label='Generate new dataset')\n                \n                ui.markdown(('A simple linear regression model $y = \\\\beta_0 + \\\\beta_1 + u$ is '\n                             'used to illustrate the notations. Here, the population parameters '\n                             '$\\\\beta_0=1.0$ and $\\\\beta_1=5.0$, and the random error $u$ follows '\n                             'a standard normal distribution $N(0, 1)$.'))\n                \n                ns = 25\n                xd = np.random.rand(ns)\n                yd = 1 + 5*xd + np.random.normal(size=ns)\n                samples = reactive.value((xd, yd))\n\n                @reactive.effect\n                @reactive.event(input.data_gen)\n                def generate_data():\n                    \n                    xd = np.random.rand(ns)\n                    yd = 1 + 5*xd + np.random.normal(size=ns)\n                    samples.set((xd, yd))\n\n                @render.plot\n                @reactive.event(input.display_reg, input.data_gen)\n                def update_reg_plot():\n                    \n                    xd, yd = samples.get()\n                    res = linregress(xd, yd)\n                    beta0, beta1 = res.intercept, res.slope\n                    \n                    displays = input.display_reg()\n\n                    fig, ax = plt.subplots()\n                    ax.scatter(xd, yd, color='r', alpha=0.6, label='The sample data')\n                    if notations[0] in displays:\n                        ax.plot([0, 1], [1, 6], color='m', linewidth=1.5, linestyle='--',\n                                label='PRF $\\\\mathbb{E}(y|x_1) = \\\\beta_0 + \\\\beta_1 x_1$')\n                    if notations[1] in displays:\n                        #beta0_str = f'{beta0:.4f}'\n                        #beta1_str = f'{beta1:.4f}'\n                        ax.plot([0, 1], [beta0, beta0+beta1], color='b', linewidth=1.5,\n                                label='SRF $\\\\hat{y} = \\\\hat{\\\\beta}_0 + \\\\hat{\\\\beta}_1 x_1$')\n                        ax.text(0.65, 0.55, f'$\\\\hat{{\\\\beta}}_0={beta0:.4f}$\\n', fontsize=12)\n                        ax.text(0.65, -0.25, f'$\\\\hat{{\\\\beta}}_1={beta1:.4f}$\\n', fontsize=12)\n                    if notations[2] in displays:\n                        ax.vlines(xd, yd, beta0+beta1*xd, color='k', linewidth=1,\n                                  label='Residuals')\n                    ax.legend(fontsize=11, loc='upper left')\n                    ax.set_xlabel('Independent variable $x_1$', fontsize=11)\n                    ax.set_ylabel('Dependent variable $y$', fontsize=11)\n                    ax.grid()\n                    ax.set_ylim([-1.7, 8.7])\n                    ax.set_xlim([-0.03, 1.03])\n\n                    return fig\n        \n        ui.markdown('<br>')\n        ui.markdown('### Goodness-of-Fit')\n        ui.markdown(('The $R^2$-value, sometimes called the **coefficient of determination** of '\n                     'a regression model, defined below, is a measure of how well the model fits '\n                     'the observed data.\\n'\n                     '$$\\n'\n                     'R^2 = \\\\frac{\\\\text{SSE}}{\\\\text{SST}} = 1-\\\\frac{\\\\text{SSR}}{\\\\text{SST}}\\n'\n                     '$$\\n'\n                     'where\\n'\n                     '- SST: the **total sum of squares**, which measures the total variations '\n                     'of the sample data $y_i$.\\n'\n                     '- SSE: the **explained sum of squares**, which measures the variation in '\n                     'the fitted value $\\\\hat{y}_i$.\\n'\n                     '- SSR: the **residual sum of squares**, which measures the variation in '\n                     'the residuals $\\\\hat{u}_i$.'))\n\n        with ui.card(height='650px'):\n            ui.card_header('Illustration of the $R^2$ value', style=chd_style)\n            with ui.layout_sidebar():\n                with ui.sidebar(bg='#f8f8f8', width='360px'):\n                    choices = ['SST', 'SSE', 'SSR']\n                    ui.input_radio_buttons(id='r_square_comp', label='Components of variations:',\n                                           choices=choices, selected=choices[0])\n                    ui.input_slider(id='error_scale', label='Scale of the error term:',\n                                    min=0.5, max=2.5, value=1.0, step=0.01)\n\n                ui.markdown(('The data visual below is used to illustrate that the variation '\n                             'components follow the equation $\\\\text{SST} = \\\\text{SSE} + '\n                             '\\\\text{SSR}$.'))\n                \n                @render.plot\n                @reactive.event(input.r_square_comp, input.error_scale, input.data_gen)\n                def update_rsquare_plot():\n                    \n                    xd, yd = samples.get()\n                    escale = input.error_scale()\n                    err = (yd - 1 - 5*xd)\n                    yd = 1 + 5*xd +  err*escale\n                    res = linregress(xd, yd)\n                    beta0, beta1, rsquare = res.intercept, res.slope, res.rvalue**2\n                    \n                    fig, ax = plt.subplots()\n                    ax.scatter(xd, yd, color='r', alpha=0.6, label='The sample data')\n                    ax.plot([0, 1], [beta0, beta0+beta1], color='b', linewidth=1.5,\n                            label='SRF $\\\\hat{y} = \\\\hat{\\\\beta}_0 + \\\\hat{\\\\beta}_1 x_1$')\n                    ybar = yd.mean()\n                    ax.plot([0, 1], np.array([1, 1])*ybar,\n                            color='m', linewidth=1.5, linestyle='--',\n                            label='Sample average $\\\\bar{y}$')\n                    if input.r_square_comp() == 'SST':\n                        ax.vlines(xd, yd, ybar, color='r', label='SST')\n                    if input.r_square_comp() == 'SSE':\n                        ax.vlines(xd, beta0+beta1*xd, ybar, color='g',label='SSE')\n                    if input.r_square_comp() == 'SSR':\n                        ax.vlines(xd, yd, beta0+beta1*xd, color='k', label='SSR')\n                    ax.legend(fontsize=11, loc='upper left')\n                    ax.text(0.62, -1.8, f'$R^2\\\\text{{ value}}={rsquare:.4f}$\\n', fontsize=12)\n                    sst = ((yd - yd.mean())**2).sum()\n                    ax.text(0.62, -3.2, f'$\\\\text{{SST}}={sst:.4f}$\\n', fontsize=12)\n                    sse = ((beta0 + beta1*xd - yd.mean())**2).sum()\n                    ax.text(0.62, -4.6, f'$\\\\text{{SSE}}={sse:.4f}$\\n', fontsize=12)\n                    ssr = ((yd - beta0 - beta1*xd)**2).sum()\n                    ax.text(0.62, -6, f'$\\\\text{{SSR}}={ssr:.4f}$\\n', fontsize=12)\n                    ax.set_xlabel('Independent variable $x_1$', fontsize=11)\n                    ax.set_ylabel('Dependent variable $y$', fontsize=11)\n                    ax.grid()\n                    ax.set_ylim([-5.8, 10.8])\n                    ax.set_xlim([-0.03, 1.03])\n\n                    return fig\n\n    with ui.nav_panel(\"Predictive Modeling: Regression\"):\n        ui.markdown('### Bias-Variance Tradeoff')\n        ui.markdown((\"The **bias\u2013variance tradeoff** describes the relationship between a model's \"\n                     \"complexity, the accuracy of its predictions, and how well it can make \"\n                     \"predictions on previously unseen data that were not used to train the model. \"\n                     \"In general, as we increase the number of tunable parameters in a model, it \"\n                     \"becomes more flexible, and can better fit a training data set. It is said \"\n                     \"to have lower error, or bias. However, for more flexible models, there will \"\n                     \"tend to be greater variance to the model fit each time we take a set of \"\n                     \"samples to create a new training data set. As a result, the best model\"\n                     \"performance may be achieved by miniminzing the combination of bias and\"\n                     \"variance.\"))\n        with ui.card(height='650px'):\n            ui.card_header('Bias-variance tradeoff in polynomial regression', style=chd_style)\n            with ui.layout_sidebar():\n                with ui.sidebar(bg='#f8f8f8', width='360px'):  \n                    ui.input_slider(id='polyreg_k', label='The number of polynomial terms',\n                                    min=1, max=24, value=4, step=1)\n                    displays = ['Training data', 'Test data', 'Population Regression Function']\n                    ui.input_selectize(id='polyreg_displays', label='Display options',\n                                       choices=displays, selected=displays[0], multiple=True)\n                    ui.input_action_button(id='polyreg_gen', label='Repeat the experiment')\n\n                    ui.markdown('---')\n\n                    @render.ui\n                    @reactive.event(input.polyreg_k, input.polyreg_gen)\n                    def update_mse():\n\n                        k = input.polyreg_k()\n                        samples = all_samples.get()\n\n                        train_mse = []\n                        test_mse = []\n                        coef_mag = []\n                        for current_sample in samples:\n                            train = current_sample.loc[:train_size]\n                            test = current_sample.loc[train_size:]\n                            coef = np.polyfit(train['x'], train['y'], k)\n\n                            train_fitted = np.polyval(coef, train['x'])\n                            test_fitted = np.polyval(coef, test['x'])\n                            train_mse.append(np.mean((train_fitted - train['y'])**2))\n                            test_mse.append(np.mean((test_fitted - test['y'])**2))\n                            coef_mag.append(np.linalg.norm(np.abs(coef)))\n\n                        return ui.markdown((\"```python\\n\\n\"\n                                            f\"Training MSE:    {np.mean(train_mse):.4f}\\n\\n\"\n                                            f\"Test MSE:        {np.mean(test_mse):.4f}\\n\\n\"\n                                            f\"Coef. magnitude: {np.mean(coef_mag):.4f}\\n\\n\"\n                                            \"```\")) \n\n                ui.markdown((\"A training dataset with 40 observations is generated from a \"\n                             \"mysterious function, and we use a polynomial regression model with \"\n                             \"$k$ polynomial terms to predict the value of the dependent variable. \"\n                             \"Use the experiments below to explore the tradeoff between the bias \"\n                             \"and variance of the prediction.\"))\n                \n                size = 50\n                train_size = 25\n                all_samples = reactive.value([mysterious(size)])\n\n                @reactive.effect\n                @reactive.event(input.polyreg_gen)\n                def generate_polyreg_data():\n                    \n                    content = all_samples.get()\n                    content.append(mysterious(size))\n                    all_samples.set(content)\n                \n                @reactive.effect\n                @reactive.event(input.polyreg_k)\n                def reset_polyreg_data():\n                    \n                    all_samples.set([mysterious(size)])\n\n                @render.plot\n                @reactive.event(input.polyreg_k, input.polyreg_displays, input.polyreg_gen)\n                def update_polyreg_plot():\n\n                    k = input.polyreg_k()\n                    samples = all_samples.get()    \n                    current_sample = samples[-1]\n\n                    train = current_sample.loc[:train_size]\n                    test = current_sample.loc[train_size:]\n                    coef = np.polyfit(train['x'], train['y'], k)\n                    \n                    fig, ax = plt.subplots()\n                    xs = np.arange(0, 1.01, 0.01)\n                    ys = np.polyval(coef, xs)\n                    ax.plot(xs, ys, color='m', linewidth=1.5, alpha=0.8,)\n                    if displays[0] in input.polyreg_displays():\n                        ax.scatter(train['x'], train['y'], \n                                   color='b', alpha=0.4, label='Training data')\n                    if displays[1] in input.polyreg_displays():\n                        ax.scatter(test['x'], test['y'], \n                                   color='r', alpha=0.4, label='Test data')\n                    if displays[2] in input.polyreg_displays():\n                        ax.plot(xs, ((1.2 - 0.2*xs) * np.sin(11*xs) + 4*xs) * 4,\n                                color='g', linewidth=2, label='PRF')\n                    for prior in samples[:-1]:\n                        train = prior.loc[:train_size]\n                        coef = np.polyfit(train['x'], train['y'], k)\n                        ys = np.polyval(coef, xs)\n                        ax.plot(xs, ys, color='k', linewidth=1, linestyle='--', alpha=0.2)\n\n                    ax.set_xlabel('Predictor variable $x$', fontsize=11)\n                    ax.set_ylabel('Predicted variable $y$', fontsize=11)\n                    ax.set_ylim([-2.5, 27.5])\n                    ax.legend(fontsize=11, loc='upper left')\n\n                    return fig\n\n\n    # with ui.nav_panel(\"Predictive Modeling: Classification\"):\n    #     ui.markdown('Under development.')\n    \n    with ui.nav_panel(\"About\"):\n        ui.markdown('Under development.')\n\n", "type": "text"}, {"name": "requirements.txt", "content": "shiny\npandas\nnumpy\nscipy\n\n", "type": "text"}]